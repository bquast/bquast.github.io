largedb=data.frame(Z=vY,X1=vX1,X2=vX2)
head(largedb,16)
glm(Z~X1+X2,family=binomial,data=largedb)
exp(lm(Z~X1+X2,data=largedb))
lin <- lm(Z~X1+X2,data=largedb)
str(lin)
ols <- function (y, X, intercept=TRUE) {
if (intercept) X <- cbind(1, X)
solve(t(X)%*%X) %*% t(X)%*%y # solve for beta
}
ols(vY, cbind(vX1, vX2), intercept = TRUE)
exp(ols(vY, cbind(vX1, vX2), intercept = TRUE))
ols(vY, cbind(vX1, vX2), intercept = TRUE) / (1 - (ols(vY, cbind(vX1, vX2), intercept = TRUE)))
glm(Z~X1+X2,family=binomial,data=largedb)
exp(ols(vY, cbind(vX1, vX2), intercept = TRUE))
1 / 1+ exp(-(ols(vY, cbind(vX1, vX2), intercept = TRUE)))
dat <- data.frame()
fix(dat)
View(dat)
hours <- 	c(0.50,	0.75,	1.00,	1.25,	1.50,	1.75,	1.75,	2.00,	2.25,	2.50,	2.75,	3.00,	3.25,	3.50,	4.00,	4.25,	4.50,	4.75,	5.00,	5.50)
Pass <- c(	0,	0,	0,	0,	0,	0,	1,	0,	1,	0,	1,	0,	1,	0,	1,	1,	1,	1,	1,	1)
glm(Pass ~ hours, family = binomial)
pass <- c(	0,	0,	0,	0,	0,	0,	1,	0,	1,	0,	1,	0,	1,	0,	1,	1,	1,	1,	1,	1)
pass <- c(	0,	0,	0,	0,	0,	0,	1,	0,	1,	0,	1,	0,	1,	0,	1,	1,	1,	1,	1,	1)
rm(Pass)
glm(pass ~ hours, family = binomial)
1 / 1+ exp(-(ols(pass, hours, intercept = TRUE)))
1 / (1 + exp(-(ols(pass, hours, intercept = TRUE)))
)
1 / 1 + exp(-(-4.0777+1.5046*2) )
1 / ( 1 + exp(-(-4.0777+1.5046*2) ) )
lm(log(pass) ~ hours)
ln(pass)
log(pass)
?log
lm(pass ~ log(hours))
lm(pass ~ log(1/hours))
lm(pass ~ log(1-(1/hours) ))
lm(pass ~ log(hours)/(1-log(hours) ) )
lm(pass ~ 1/(1+exp(hours) ) )
?mle
x <- 0:10
y <- c(26, 17, 13, 12, 20, 5, 9, 8, 5, 4, 8)
nLL <- function(lambda) -sum(stats::dpois(y, lambda, log = TRUE))
fit0 <- mle(nLL, start = list(lambda = 5), nobs = NROW(y))
library(stats4)
fit0 <- mle(nLL, start = list(lambda = 5), nobs = NROW(y))
fit0
mydata = read.csv(url('http://www.ats.ucla.edu/stat/r/dae/binary.csv'))
library(haven)
?read_sas(url('binary.sas7bdat'))
sas<-read_sas(url('binary.sas7bdat'))
download.file('http://www.ats.ucla.edu/stat/sas/dae/binary.sas7bdat')
sas<-read_sas(url('http://www.ats.ucla.edu/stat/sas/dae/binary.sas7bdat'))
sas<-read_sas(url('http://www.ats.ucla.edu/stat/sas/dae/binary.sas7bdat'), path=".")
download.file('http://www.ats.ucla.edu/stat/sas/dae/binary.sas7bdat', destfile = "binary.sas7bdat")
binary <- read_sas('binary.sas7bdat')
binary <- read_sas('binary.sas7bdat')
glm(admit~gre+gpa+as.factor(rank), family=binomial, data=binary)
head(binary)
names(binary) <- tolower(names(binary))
head(binary)
glm(admit~gre+gpa+as.factor(rank), family=binomial, data=binary)
mle.logreg = function(fmla, data)
{
# Define the negative log likelihood function
logl <- function(theta,x,y){
y <- y
x <- as.matrix(x)
beta <- theta[1:ncol(x)]
# Use the log-likelihood of the Bernouilli distribution, where p is
# defined as the logistic transformation of a linear combination
# of predictors, according to logit(p)=(x%*%beta)
loglik <- sum(-y*log(1 + exp(-(x%*%beta))) - (1-y)*log(1 + exp(x%*%beta)))
return(-loglik)
}
# Prepare the data
outcome = rownames(attr(terms(fmla),"factors"))[1]
dfrTmp = model.frame(data)
x = as.matrix(model.matrix(fmla, data=dfrTmp))
y = as.numeric(as.matrix(data[,match(outcome,colnames(data))]))
# Define initial values for the parameters
theta.start = rep(0,(dim(x)[2]))
names(theta.start) = colnames(x)
# Calculate the maximum likelihood
mle = optim(theta.start,logl,x=x,y=y,hessian=T)
out = list(beta=mle$par,vcov=solve(mle$hessian),ll=2*mle$value)
}
mydata$rank = factor(mydata$rank) #Treat rank as a categorical variable
binary$rank = factor(binary$rank) #Treat rank as a categorical variable
fmla = as.formula("admit~gre+gpa+rank") #Create model formula
mylogit = mle.logreg(fmla, binary) #Estimate coefficients
mylogit
glm(admit~gre+gpa+as.factor(rank), family=binomial, data=binary)
mle.logreg = function(fmla, data)
{
# Define the negative log likelihood function
logl <- function(theta,x,y){
y <- y
x <- as.matrix(x)
beta <- theta[1:ncol(x)]
# Use the log-likelihood of the Bernouilli distribution, where p is
# defined as the logistic transformation of a linear combination
# of predictors, according to logit(p)=(x%*%beta)
loglik <- sum(-y*log(1 + exp(-(x%*%beta))) - (1-y)*log(1 + exp(x%*%beta)))
return(-loglik)
}
# Prepare the data
outcome = rownames(attr(terms(fmla),"factors"))[1]
dfrTmp = model.frame(data)
x = as.matrix(model.matrix(fmla, data=dfrTmp))
y = as.numeric(as.matrix(data[,match(outcome,colnames(data))]))
# Define initial values for the parameters
theta.start = rep(0,(dim(x)[2]))
names(theta.start) = colnames(x)
# Calculate the maximum likelihood
mle = optim(theta.start,logl,x=x,y=y,hessian=F)
# Obtain regression coefficients
beta = mle$par
# Calculate the Information matrix
# The variance of a Bernouilli distribution is given by p(1-p)
p = 1/(1+exp(-x%*%beta))
V = array(0,dim=c(dim(x)[1],dim(x)[1]))
diag(V) = p*(1-p)
IB = t(x)%*%V%*%x
# Return estimates
out = list(beta=beta,vcov=solve(IB),dev=2*mle$value)
}
mylogit = mle.logreg(fmla, binary) #Estimate coefficients
mylogit
glm(admit~gre+gpa+as.factor(rank), family=binomial, data=binary)
?optim
logitdata <- read.table("~/Downloads/ps206data1b.txt", header=T, sep="\t")
logitdata <- as.data.frame(logitdata)
attach(logitdata)
logitmodel <- glm(itn ~  altitude, family=binomial(link="logit"), data=logitdata)
summary(logitmodel)
logitmodel2 <- glm(itn ~  1, family=binomial(link="logit"), data=logitdata)
summary(logitmodel2)
summary(logitmodel)
log(mean(itn)) - log(1 - mean(itn))
X <- cbind(1, altitude)
y <- itn
K <- as.numeric(ncol(X))
startvalue.model <- lm(itn ~ altitude)
startv <- startvalue.model$coefficients
logit.lf <- function(beta) {
exb <- exp(X%*%beta)
prob1 <- exb/(1+exb)
logexb <- log(prob1)
y0 <- 1 - y
logexb0 <- log(1 - prob1)
yt <- t(y)
y0t <- t(y0)
logl <- -sum(yt%*%logexb + y0t%*%logexb0)
return(logl)
}
logit.gr <- function(beta) {
grad <- beta*0
exb <- exp(X%*%beta)
prob1 <- exb/(1+exb)
for (k in 1:K) {
grad[k] <- sum(X[,k] * (y - prob1))
}
return(-grad)
}
logitmodel <- optim(startv, logit.lf, gr=logit.gr, method="BFGS", control=list(trace=TRUE, REPORT=1), hessian=TRUE)
coeffs <- logitmodel$par
coeffs
summary(logitmodel)
glm(itn ~  altitude, family=binomial(link="logit"), data=logitdata)
coeffs
?optim
library(servr)
jekyll()
X = matrix(c(0,0,1,0,1,1,1,0,1,1,1,1), nrow=4, byrow=TRUE)
y = matrix(c(0,1,1,0),nrow=4)
x
X
y
syn0 = matrix(runif(n = 12, min=-1, max=1), nrow=3)
syn1 = matrix(runif(n =  4, min=-1, max=1), nrow=4)
syn0
syn0 = matrix(runif(n = 12, min=-1, max=1), nrow=3)
syn0
for (j in 1:60000) {
# Feed forward through layers 0, 1, and 2
l0 = X
l1 = nonlin(l0%*%syn0)
l2 = nonlin(l1%*%syn1)
# how much did we miss the target value?
l2_error = y - l2
if (j %% 10000 == 0)
print(paste("Error:", mean(abs(l2_error))))
# in what direction is the target value?
# were we really sure? if so, don't change too much.
l2_delta = l2_error*nonlin(l2,deriv=TRUE)
# how much did each L1 value contribute to the error (according to the weights)?
l1_error = l2_delta %*% t(syn1)
# in what direction is the target l1?
# were we really sure? if so, don't change too much.
l1_delta = l1_error * nonlin(l1, deriv=TRUE)
syn1 = syn1 + t(l1) %*% l2_delta
syn0 = syn0 + t(l0) %*% l1_delta                     }
nonlin = function(x,deriv=FALSE) {
if(deriv==TRUE)
return( x*(1-x) )
return( 1/(1+exp(-x)) )
}
nonlin
for (j in 1:60000) {
# Feed forward through layers 0, 1, and 2
l0 = X
l1 = nonlin(l0%*%syn0)
l2 = nonlin(l1%*%syn1)
# how much did we miss the target value?
l2_error = y - l2
if (j %% 10000 == 0)
print(paste("Error:", mean(abs(l2_error))))
# in what direction is the target value?
# were we really sure? if so, don't change too much.
l2_delta = l2_error*nonlin(l2,deriv=TRUE)
# how much did each L1 value contribute to the error (according to the weights)?
l1_error = l2_delta %*% t(syn1)
# in what direction is the target l1?
# were we really sure? if so, don't change too much.
l1_delta = l1_error * nonlin(l1, deriv=TRUE)
syn1 = syn1 + t(l1) %*% l2_delta
syn0 = syn0 + t(l0) %*% l1_delta                     }
data(swiss)
str(swiss)
m1 <- formula(Fertility ~ Agriculture + Examination + Education + Catholic + Infant.Mortality)
lm1 <- lm(formula=m1, data=swiss)
summary(lm1)
library(nnet)
nn1 <- nnet(formula=m1, data=swiss, size=0, skip=TRUE, linout=TRUE)
summary(nn1)
library(NeuralNetTools)
plotnet(nn1)
library(neuralnet)
nn1a <- neuralnet(formula=m1, data=swiss, hidden=0, linear.output=TRUE)
plot(nn1a, rep='best')
plot(nn1a, rep='best')
?pow
?power
@^8
2^8
?`^`
# no importing here
set.set(0)
# compute sigmoid nonlinearity
sigmoid = function(x) {
output = 1 / (1+exp(-x))
return(output)            }
# convert output of sigmoid function to its derivative
signmoid_output_to_derivative = function(output) {
return( output*(1-output) )                      }
# training dataset generation
int2binary = array()
binary_dim = 8
largest_number = 2^binary_dim
?readBin
as.integer(intToBits(5))
intToBits(5)
t(as.integer(intToBits(5)))
as.integer(intToBits(5))
sapply(5,function(x){ as.integer(intToBits(x))})
?sapply
sapply(5,function(x){ as.integer(intToBits(x))})
as.integer(intToBits(5))
as.vector(intToBits(5))
as.integer(intToBits(5))
?intToBits
rawToBits(5)
rev(as.integer(intToBits(5)))
intToBits
binary = function(x) {
rev(as.integer(intToBits())) }
binary(6)
binary = function(x) {
rev(as.integer(intToBits(x))) }
binary = function(x) {
binary(6)
?tail
tail(binary(256))
binary(356)
binary(256)
binary(255)
tail(binary(255), binary_dim=8)
binary = function(x) {
tail(rev(as.integer(intToBits(x))), binary_dim) }
binary(256)
range(256)
?range
for (in in 1:largest_number) {
int2binary[i] = binary[i]   }
for (i in 1:largest_number) {
int2binary[i] = binary[i]   }
for (i in 1:largest_number) {
int2binary[i] = binary[i]   }
binary(5)
matrix(runif(n = 3, min=-1, max=1), nrow=3)
?runif
synapse_0 = matrix(runif(n = input_dim*hidden_dim, min=-1, max=1), nrow=hidden_dim)
# input variables
alpha = 0.1
input_dim = 2
hidden_dim = 16
output_dim = 1
# initialize neural network weights
synapse_0 = matrix(runif(n = input_dim*hidden_dim, min=-1, max=1), nrow=hidden_dim)
synapse_0
synapse_0 = matrix(runif(n = input_dim*hidden_dim, min=-1, max=1), nrow=input_dim)
synapse_0
synapse_0 = matrix(runif(n = input_dim*hidden_dim, min=-1, max=1), nrow=input_dim)
synapse_1 = matrix(runif(n = hidden_dim*output_dim, min=-1, max=1), nrow=output_dim)
syanpse_h = matrix(runif(n = hidden_dim*hidden_dim, min=-1, max=1), nrow=hidden_dim)
synapse_0
a <- (synapse_0 <- NULL)
a
?runif
?rint
?vector
vector(length=8)
a_int = sample(1:largest_number/2) # int version
a_int = sample(1:largest_number/2, 1) # int version
a = int2binary(a_int) # binary encoding
int2binary = function(x) {
tail(rev(as.integer(intToBits(x))), binary_dim) }
a = int2binary(a_int) # binary encoding
a
d = matrix(0, nrow = 1, ncol = binary_dim)
layer_1_values <- list(layer_1_values, vector(lengt=hidden_dim))
layer_1_values = list()
layer_1_values <- list(layer_1_values, vector(lengt=hidden_dim))
layer_1_values
layer_1_values = list()
layer_1_values <- c(layer_1_values, vector(lengt=hidden_dim))
layer_1_values
layer_1_values <- c(layer_1_values, vector(length=hidden_dim))
layer_1_values = list()
layer_1_values <- c(layer_1_values, vector(length=hidden_dim))
layer_1_values
layer_1_values <- c(layer_1_values, vector(length=hidden_dim))
layer_1_values
layer_1_values = list()
layer_1_values[i] <- vector(length=hidden_dim)
a[5]
typeof(rbind(a[5],b[5]))
b_int = sample(1:largest_number/2)
b = int2binary(b_int)
# true answer
c_int = a_int + b_int
c = int2binary(c_int)
# where we'll store our best guesss (binary encoded)
d = matrix(0, nrow = 1, ncol = binary_dim)
overallError = 0
layer_2_deltas = list()
layer_1_values = list()
layer_1_values <- c(layer_1_values, vector(length=hidden_dim)) # this doesn't work yet
typeof(rbind(a[5],b[5]))
rbind(a[5],b[5])
str(rbind(a[5],b[5]))
t(rbind(a[5],b[5]))
# hidden layer (input ~+ prev_hidden)
layer_1 = sigmoid(X%*%synapse_0) +
(layer_1_values %*% syanpse_h)
X = rbind(a[binary_dim - position],b[binary_dim - position])
position(1)
position = 1
# generate input and output
X = rbind(a[binary_dim - position],b[binary_dim - position])
y = c[binary_dim - position]
# hidden layer (input ~+ prev_hidden)
layer_1 = sigmoid(X%*%synapse_0) +
(layer_1_values %*% syanpse_h)
?`%*%
`
?`%*%`
# no importing here
set.set(0)
# compute sigmoid nonlinearity
sigmoid = function(x) {
output = 1 / (1+exp(-x))
return(output)            }
# convert output of sigmoid function to its derivative
signmoid_output_to_derivative = function(output) {
return( output*(1-output) )                      }
# training dataset generation
# int2binary =
binary_dim = 8
largest_number = 2^binary_dim
int2binary = function(x) {
tail(rev(as.integer(intToBits(x))), binary_dim) }
# for (i in 1:largest_number) {
#  int2binary[i] = binary[i]   }
# input variables
alpha = 0.1
input_dim = 2
hidden_dim = 16
output_dim = 1
# initialize neural network weights
synapse_0 = matrix(runif(n = input_dim*hidden_dim, min=-1, max=1), nrow=input_dim)
synapse_1 = matrix(runif(n = hidden_dim*output_dim, min=-1, max=1), nrow=output_dim)
synapse_h = matrix(runif(n = hidden_dim*hidden_dim, min=-1, max=1), nrow=hidden_dim)
synapse_0_update = matrix(0, nrow = input_dim, ncol = hidden_dim)
synapse_1_update = matrix(0, nrow = hidden_dim, ncol = output_dim)
synapse_h_update = matrix(0, nrow = hidden_dim, ncol = hidden_dim)
position = 1
# generate input and output
X = rbind(a[binary_dim - position],b[binary_dim - position])
y = c[binary_dim - position]
# generate a simple addition problem (a + b = c)
a_int = sample(1:largest_number/2, 1) # int version
a = int2binary(a_int) # binary encoding
b_int = sample(1:largest_number/2)
b = int2binary(b_int)
# true answer
c_int = a_int + b_int
c = int2binary(c_int)
# where we'll store our best guesss (binary encoded)
d = matrix(0, nrow = 1, ncol = binary_dim)
overallError = 0
layer_2_deltas = matrix()
layer_1_values = matrix()
layer_1_values <- c(layer_1_values, vector(length=hidden_dim)) # this doesn't work yet
layer_1_values = matrix()
layer_1_values <- c(layer_1_values, matrix(0, nrow=1, ncol=hidden_dim)) # this doesn't work yet
layer_1_values <- c(layer_1_values, matrix(0, nrow=1, ncol=hidden_dim)) # this doesn't work yet
layer_1_values <- c(layer_1_values, matrix(0, nrow=1, ncol=hidden_dim)) # this doesn't work yet
layer_1_values = matrix()
layer_1_values <- rbind(layer_1_values, matrix(0, nrow=1, ncol=hidden_dim)) # this doesn't work yet
layer_1_values <- matrix(0, 1, 8)
layer_1 = sigmoid(X%*%synapse_0) +
(layer_1_values %*% synapse_h)
# generate input and output
X = rbind(a[binary_dim - position],b[binary_dim - position])
y = c[binary_dim - position]
# hidden layer (input ~+ prev_hidden)
layer_1 = sigmoid(X%*%synapse_0) +
(layer_1_values %*% synapse_h)
dim(X)
layer_1 = sigmoid((X%*%synapse_0) +
(layer_1_values %*% synapse_h))
(X%*%synapse_0)
dim(X)
dim(synapse_0)
t(X)%*%synapse_0
X = cbind(a[binary_dim - position],b[binary_dim - position])
(X%*%synapse_0)
layer_1 = sigmoid((X%*%synapse_0) +
(layer_1_values %*% synapse_h))
dim(layer_1_values)
dim(synapse_h)
layer_1_values = matrix(0 , nrow=1, ncol = hidden_dim)
layer_1_values = rbind(layer_1_values, matrix(0, nrow=1, ncol=hidden_dim)) # this doesn't work yet
layer_1 = sigmoid((X%*%synapse_0) +
(layer_1_values[position-1,] %*% synapse_h))
position
layer_1 = sigmoid((X%*%synapse_0) +
(layer_1_values[position,] %*% synapse_h))
?Abs
?abs
?round
library(servr)
jekyll(serve = FALSE)
jekyll(serve = FALSE)
jekyll(serve = FALSE)
jekyll(serve = FALSE)
library(wiod)
library(learNN)
help('learNN')
help(package = 'learNN')
help('learnn')
help(package = 'learNN')
jekyll(serve = FALSE)
jekyll(serve = FALSE)
data(wiod95)
ls()
str(ls())
ls()
str()
jekyll(serve = FALSE)
jekyll(serve = FALSE)
jekyll(serve = FALSE)
jekyll(serve = FALSE)
jekyll(serve = FALSE)
library(servr)
jekyll()
jekyll(serve = FALSE)
jekyll(serve = FALSE)
jekyll(serve = FALSE)
library(servr)
jekyll()
library(servr)
jekyll(serve = FALSE)
jekyll(serve = FALSE)
jekyll()
library(servr)
jekyll(serve = FALSE)
jekyll(serve = FALSE)
jekyll(serve = FALSE)
library(rnn)
?rnn
?ggplotly
library(plotly)
?ggplotly
install.packages("servr")
library(servr)
jekyll()
library(servr)
jekyll()
