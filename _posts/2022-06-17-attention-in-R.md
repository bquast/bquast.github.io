---
layout: single
title: "Attention in R"
permalink: attention-in-R 
tags: [R, attention, self-attention, transformers, machine learning]
---

This post describes how to implement the [attention mechanism](https://en.m.wikipedia.org/wiki/Attention_(machine_learning)) that forms the basis of [transformers](https://en.m.wikipedia.org/wiki/Transformer_(machine_learning_model)) in the [R language](https://en.m.wikipedia.org/wiki/R_(programming_language)).

The code is translated from the [Python](https://www.python.org/) original by [Stefania Cristina](https://scholar.google.com/citations?user=ncHZ0mwAAAAJ&hl=en) ([University of Malta](https://www.um.edu.mt/profile/stefaniacristina)) in her post [The Attention Mechanism from Scratch](https://machinelearningmastery.com/the-attention-mechanism-from-scratch/)

We begin by generating encoder representations of four different words.

```
# encoder representations of four different words
word_1 = matrix(c(1,0,0), nrow=1)
word_2 = matrix(c(0,1,0), nrow=1)
word_3 = matrix(c(1,1,0), nrow=1)
word_4 = matrix(c(0,0,1), nrow=1)
```

Next we stack the word embeddings into a single array (in this case a matrix).
```
# stacking the word embeddings into a single array
words = rbind(word_1,
              word_2,
              word_3,
              word_4)
```

Next we generate random integers on the domain [0,3].
```
# generating the weight matrices
set.seed(0)
W_Q = matrix(floor(runif(9, min=0, max=3)),nrow=3,ncol=3)
W_K = matrix(floor(runif(9, min=0, max=3)),nrow=3,ncol=3)
W_V = matrix(floor(runif(9, min=0, max=3)),nrow=3,ncol=3)
```

In order to keep the numbers the same as in the original Python code, you can overwrite the randomly generated values with the values as they were generated by Python.
```
# redefine matrices to match random numbers generated by Python in the original code
W_Q = matrix(c(2,0,2,
               2,0,0,
               2,1,2),
             nrow=3,
             ncol=3,
             byrow = TRUE)
W_K = matrix(c(2,2,2,
               0,2,1,
               0,1,1),
             nrow=3,
             ncol=3,
             byrow = TRUE)
W_V = matrix(c(1,1,0,
               0,1,1,
               0,0,0),
             nrow=3,
             ncol=3,
             byrow = TRUE)
```

Next we generate the Queries (Q), Keys (K), and Values (V). The `%*%` operator performs the matrix multiplication. You can view the R help page using `help('%*%')`
```
# generating the queries, keys and values
Q = words %*% W_Q
K = words %*% W_K
V = words %*% W_V
```

Following this, we score the Queries (Q) against the Key (K) vectors.
```
# scoring the query vectors against all key vectors
scores = Q %*% t(K)
```

We now need to find the maximum value for each row of the `scores` matrix. The easiest way to do this, is using the `rowMaxs()` function from the [matrixStats](https://cran.r-project.org/package=matrixStats) package.

The next code block checks if the `matrixStats` package is already installed, if it is not, it will install the package. Next the package is loaded.
```
# check if the matrixStats package needs to be install, if so, do so
# then load the package (to be used for rowMaxs
if (!require('matrixStats')) install.packages('matrixStats')
library(matrixStats)
```

We now calculate the maximum value for each row, and preserve the structure (i.e. the 4 rows, now with only one column which contains the maximum value for the corresponding row).
```
# calculate the max for each row of the scores matrix
maxs = as.matrix(rowMaxs(scores))
```

The weights matrix will be populated using a `for loop` (see `help('for')`). Since the loop does not edit the dimensions of the matrix, we generate zero matrix (i.e. a values are set to 0) beforehand, which we then populate using the `for loop`.
```
# initialive weights matrix
weights = matrix(0, nrow=4, ncol=4)
```

We now populate the `weights` matrix using the `for loop`.
```
# computing the weights by a softmax operation
for (i in 1:dim(scores)[1]) {
  weights[i,] = exp((scores[i,]-maxs[i,]) / ncol(K) ^ 0.5)/sum(exp((scores[i,]-maxs[i,]) / ncol(K) ^ 0.5))
}
```

Finally, we compute the attention as a weighted sum of the value vectors.
```
# computing the attention by a weighted sum of the value vectors
attention = weights %*% V
```

Now we can view the results using:
```
print(attention)
```

This gives:
```
          [,1]     [,2]      [,3]
[1,] 0.9852202 1.741741 0.7565203
[2,] 0.9096526 1.409653 0.5000000
[3,] 0.9985123 1.758493 0.7599811
[4,] 0.9956039 1.904073 0.9084692
```

As you can see, these are the same values as those computed in Python in the original post. 

The complete code is also available as a Gist on [GitHub](https://gist.github.com/bquast/169c42090e4337c5f4023ac46ce694f2).

