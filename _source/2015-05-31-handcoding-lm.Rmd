---
layout: post
title: "Hand Coding the Linear Model"
tags: [R, hand coding, linear model, lm, ols]
permalink: handcoding-lm
---

> In order to understand statistics, you have to do the calculations yourself

Warnings such as these are often given in statistics courses and for good reason too!
Doing the work yourself really cements the understanding of statistics.

Yet when it comes to econometrics, the main take aways from the workshops are primarily in terms of the **syntax** of yet another computer program.

In this series of post titled: **handcoded**, I show how many workhorses of the econometrician's toolbox can be implemented in a very simple manner.
This manual implementation greatly helps in keeping an understanding of what actually happens when we call e.g. `MCMC..` with a few parameters.


The Linear Model
----------------------
Then using the **Ordinary Least Squares** approach to solving a model, we start with the following equation of the OLS model for a univariate regression.

$$
y_i = \beta_0 \beta_1 x_1 + \epsilon
$$

For \$$ \beta_1 $$, this [gives us](https://en.wikipedia.org/wiki/Simple_linear_regression#Fitting_the_regression_line)
(hat denotes the estimator, bar denotes the mean):

$$
\hat{\beta_1} = \frac{ \sumˆn_{i=1} (x_i - \bar{x} )(y_i - \bar{y} ) }{(x_i - \bar{x})ˆ2 }
$$

We start by loading a basic data set.

```{r}
data("iris")
```

Inspect the data set.

```{r}
summary(iris)
```

Assign our variables to objects (in the global environment)

```{r}
y <- iris$Petal.Length
x <- iris$Petal.Width
```

We can now compute the \$$\hat{ \beta_1 }$$:

```{r}
x_m <- mean(x) # x bar in our equation
y_m <- mean(y) # y bar in our equation
numerator   <- sum( (x-x_m) * (y-y_m) )
denominator <- sum( (x-x_m)*(x-x_m) ) 
beta1_hat   <- numerator / denominator
```

Using \$$ \hat{ \beta_1 } $$ we can now compute \\$ \hat{beta_0} $$

```{r}
beta0_hat <- y_m - ( beta1_hat * x_m )
```

Lets check this using the built in command.

```{r}
lm( y ~ x )
```



The matrix model
------------------
In matrix form we can specify our general equation as:

$$
y = \Beta X + \epsilon
$$

From which we can derive our estimator:

$$
\Beta = (X^T *X)^{-1} * (X^T*y)
$$


The matrix estimation
-----------------------

Use the built in command.

```{r}
lm( y ~ x -1 ) # the -1 ensures that we don't have an intercept
```

Now we estimate our beta ourselves, the function used to invert is called `solve()`.

```{r}
solve(t(x)%*%x) %*% t(x)%*%y
```

Now lets estimate with an intercept

```{r}
lm( y ~ x )
```

To hand code this, we need to add a vector of ones (`1`s).
Note that the single `1` that we are binding to the vector `X` will be repeated until it is the same length.

```{r}
XI <- cbind(x, 1) # create a new object XI (I for Intercept)
head(XI) # inspect the first few rows
```

We also have to discuss the above `solve()` function, `^-1` is not correct syntax for a matrix inversion. In the above case it would still work correctly because our `X` matrix is in fact a vector. If we pre-multiply this vector with the transpose of itself, we obtain a scalar.

```{r}
dim( t(x) %*% x )
```

However, for matrices wider than one column this is not the case.

```{r}
dim( t(XI) %*% XI )
```

This `^-1` will invert every **individual** number in the matrix, rather than the matrix as a whole.

```{r}
(t(XI)%*%XI)^-1
```

We want to obtain to obtain the inverse of the matrix, because this will allow us to pre-multiply on both sides, eliminating `XI` on the **Right-Hand Side** (RHS).

We therefore use a different tool, the`solve()` function from the `base` package.
This function implements the [QR decomposition](https://en.wikipedia.org/wiki/QR_decomposition),
which is an efficient way of deriving an inverse of a matrix.

Now we can use this matrix to estimate a model with an intercept.

```{r}
solve( (t(XI)%*%XI) ) %*% t(XI)%*%y
```

Note that this is programmatically exactly the same the way that the `lm()` function does this.

We can suppress the automatic intercept and include our `XI` variable and we will obtain the same results.

```{r}
lm(y ~ XI -1 )
```

We have now constructed a univariate (uni**variate**) model, however, from a programmatic point of view, the hurdles of multivariate modelling have already been overcome by estimating a model with an intercept (making `X` a matrix).

It is therefore very easy to use the same method in a case with two independent variables.

```{r}
x1 <- iris$Petal.Width
x2 <- iris$Sepal.Length
y  <- iris$Petal.Length
```

we start by binding the two independent variables together (with vector of `1`s, since we want an intercept).

```{r}
XI <- cbind(x1, x2, 1)
head(XI)
```

Now we estimate our model

```{r}
solve( (t(XI)%*%XI) ) %*% t(XI)%*%y
```

And that's all! The leap from univariate to multivariate modelling was truly very small.

**EDIT:** in [tomorrow's post](/handcoding-lm-function) we use the method we developed here to create an easy to use function.
